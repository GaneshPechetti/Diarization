{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkxWgp375w8IdyMvXN7D0L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaneshPechetti/Diarization/blob/main/SVPD_DiarizationPrototype_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Speaker Diarizaton model_1\n",
        "  Achitecture :    \n",
        "\n",
        "        * Step 1 : loading libraries \n",
        "        * Step 2 : Passing the audio to the silero vad model (if necessary)\n",
        "        * Step 3 : Preparing the audio file\n",
        "        * Step 4 : Passing to the hugging face model\n",
        "        * Step 5 : Creating a json file using the rttm file\n",
        "        * Step 6 : Generating the audio files for every different speaker which consists of their only their voice"
      ],
      "metadata": {
        "id": "aOqFgMK-Kl1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VAD\n",
        "In this section we are using silero VAD model to for Voice Activity Detection. "
      ],
      "metadata": {
        "id": "HkADMco4OedB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchaudio\n",
        "\n",
        "SAMPLING_RATE = 16000\n",
        "\n",
        "import torch\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "from IPython.display import Audio\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "WAINAUK_Obix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_ONNX = False # change this to True if you want to test onnx model\n",
        "if USE_ONNX:\n",
        "    !pip install -q onnxruntime\n",
        "  \n",
        "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
        "                              model='silero_vad',\n",
        "                              force_reload=True,\n",
        "                              onnx=USE_ONNX)\n",
        "\n",
        "(get_speech_timestamps,\n",
        " save_audio,\n",
        " read_audio,\n",
        " VADIterator,\n",
        " collect_chunks) = utils"
      ],
      "metadata": {
        "id": "_Acu2DekOqMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLING_RATE = 16000\n",
        "audio_file = \"\" # set path to the audio file\n",
        "wav = read_audio(audio_file, sampling_rate=SAMPLING_RATE)\n",
        "# get speech timestamps from full audio file\n",
        "\n",
        "speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=SAMPLING_RATE)\n",
        "pprint(speech_timestamps)"
      ],
      "metadata": {
        "id": "rDKlG9_LOwrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge all speech chunks to one audio\n",
        "save_audio('only_speech_.wav',\n",
        "           collect_chunks(speech_timestamps, wav), sampling_rate=SAMPLING_RATE) "
      ],
      "metadata": {
        "id": "1FVk82kHO0GX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Diarization"
      ],
      "metadata": {
        "id": "vhgq9HvZOcTl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPopygJ_tFZZ"
      },
      "outputs": [],
      "source": [
        "pip install -qq https://github.com/pyannote/pyannote-audio/archive/refs/heads/develop.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the model\n",
        "from pyannote.audio import Pipeline\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\",\n",
        "                                    use_auth_token=\"create_a_token_from_hugging_face\")"
      ],
      "metadata": {
        "id": "CX4IyZWRMB6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the pipeline to an audio file\n",
        "diarization = pipeline(audio_file,max_speakers=3)"
      ],
      "metadata": {
        "id": "6kiD94y_tTBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dump the diarization output to disk using RTTM format\n",
        "with open(\"Sys_genereated.rttm\", \"w\") as rttm:\n",
        "    diarization.write_rttm(rttm)"
      ],
      "metadata": {
        "id": "ZRBBEYpZMV3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a json file from the rttm file\n"
      ],
      "metadata": {
        "id": "CEOo423nKjcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the path to the RTTM file\n",
        "rttm_file = \"\"           ## insert path to system generated rttm file\n",
        "\n",
        "# Define the path to the output JSON file\n",
        "json_file = \"/content/\"+audio_file[:-4]+\".json\"\n",
        "\n",
        "# Create an empty list to store the speaker turns\n",
        "speaker_turns = []\n",
        "\n",
        "# Open the RTTM file for reading\n",
        "with open(rttm_file, \"r\") as f:\n",
        "    # Iterate over each line in the file\n",
        "    for line in f:\n",
        "        # Split the line into fields\n",
        "        fields = line.strip().split(\" \")\n",
        "        \n",
        "        # Extract the relevant information\n",
        "        speaker_id = fields[7]\n",
        "        start_time = float(fields[3])\n",
        "        end_time = start_time + float(fields[4])\n",
        "        \n",
        "        # Create a dictionary for the speaker turn\n",
        "        speaker_turn = {\n",
        "            \"label\": speaker_id,\n",
        "            \"start\": start_time,\n",
        "            \"stop\": end_time\n",
        "        }\n",
        "        \n",
        "        # Append the speaker turn to the list\n",
        "        speaker_turns.append(speaker_turn)\n",
        "\n",
        "# Create a dictionary for the JSON object\n",
        "json_data = {\n",
        "    \"text\": str(speaker_turns)\n",
        "}\n",
        "\n",
        "# Save the JSON data to the output file\n",
        "with open(json_file, \"w\") as f:\n",
        "    json.dump(json_data, f)\n"
      ],
      "metadata": {
        "id": "DXF2uf2_Nk7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segmenting the audio file & Merging the same speaker files"
      ],
      "metadata": {
        "id": "oB08tJb2VyJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pydub"
      ],
      "metadata": {
        "id": "OPoG-F-iP9lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the path to the JSON file\n",
        "json_file_path = json_file\n",
        "\n",
        "# Open the JSON file for reading\n",
        "with open(json_file_path, \"r\") as f:\n",
        "    # Load the JSON data into a Python variable\n",
        "    json_data = json.load(f)"
      ],
      "metadata": {
        "id": "RhSYHPin5Qq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "# Extract speech segments from JSON data\n",
        "speech_segments = eval(json_data[\"text\"])\n",
        "\n",
        "# Load audio file\n",
        "audio_file = AudioSegment.from_wav(audio_file)\n",
        "\n",
        "# Create a dictionary to store segmented audio for each speaker ID\n",
        "segmented_audio_dict = {}\n",
        "\n",
        "# Iterate through speech segments\n",
        "for segment in speech_segments:\n",
        "    start_time = segment[\"start\"] * 1000  # Convert to milliseconds\n",
        "    end_time = segment[\"stop\"] * 1000  # Convert to milliseconds\n",
        "    speaker_id = segment[\"label\"]\n",
        "\n",
        "    # Segment audio file\n",
        "    segmented_audio = audio_file[start_time:end_time]\n",
        "\n",
        "    # If speaker ID already exists in the dictionary, concatenate the audio segment\n",
        "    if speaker_id in segmented_audio_dict:\n",
        "        segmented_audio_dict[speaker_id] += segmented_audio\n",
        "    # If speaker ID is new, add the audio segment as a new entry in the dictionary\n",
        "    else:\n",
        "        segmented_audio_dict[speaker_id] = segmented_audio\n",
        "\n",
        "# Merge audio segments from the same speaker ID\n",
        "for speaker_id, segmented_audio in segmented_audio_dict.items():\n",
        "    # Merge audio segments from the same speaker ID\n",
        "    merged_audio = segmented_audio[0]\n",
        "    for i in range(1, len(segmented_audio)):\n",
        "        merged_audio += segmented_audio[i]\n",
        "\n",
        "    # Update the dictionary with the merged audio\n",
        "    segmented_audio_dict[speaker_id] = merged_audio\n",
        "\n",
        "# Save merged audio for each speaker ID\n",
        "for speaker_id, merged_audio in segmented_audio_dict.items():\n",
        "    output_filename = f\"merged_audio_{speaker_id}.wav\"  # Generate output filename based on speaker ID\n",
        "    merged_audio.export(output_filename, format=\"wav\")  # Save merged audio as WAV file\n",
        "    print(f\"Merged audio for Speaker ID '{speaker_id}' saved as '{output_filename}'\")\n",
        "\n",
        "print(\"Segmentation and merging complete!\")\n"
      ],
      "metadata": {
        "id": "710ETA5NVuhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation\n",
        "##Diarization error rate\n",
        "\n",
        "* Here we used simpleder module for calculating DER.\n",
        "* rttm_file refers to the system generated RTTM file."
      ],
      "metadata": {
        "id": "OK2SXI6Fv54g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install simpleder"
      ],
      "metadata": {
        "id": "oxu1dhzRwHxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import simpleder\n",
        "\n",
        "# Define the path to the RTTM file\n",
        "rttm_file = \"\" #path to system rttm file\n",
        "\n",
        "# Define the path to the hypothesis RTTM file (same as the reference for this example)\n",
        "hyp_file = \"\" #path to the grouth truth rttm file\n",
        "\n",
        "# Open the RTTM file for reading\n",
        "with open(rttm_file, \"r\") as f:\n",
        "    # Create an empty list to store the reference speaker turns\n",
        "    ref_turns = []\n",
        "\n",
        "    # Iterate over each line in the file\n",
        "    for line in f:\n",
        "        # Split the line into fields\n",
        "        fields = line.strip().split(\" \")\n",
        "\n",
        "        # Extract the relevant information\n",
        "        speaker_id = fields[7]\n",
        "        start_time = float(fields[3])\n",
        "        end_time = start_time + float(fields[4])\n",
        "\n",
        "        # Append the speaker turn to the list\n",
        "        ref_turns.append((speaker_id, start_time, end_time))\n",
        "\n",
        "# Open the hypothesis RTTM file for reading\n",
        "with open(hyp_file, \"r\") as f:\n",
        "    # Create an empty list to store the hypothesis speaker turns\n",
        "    hyp_turns = []\n",
        "\n",
        "    # Iterate over each line in the file\n",
        "    for line in f:\n",
        "        # Split the line into fields\n",
        "        fields = line.strip().split(\" \")\n",
        "\n",
        "        # Extract the relevant information\n",
        "        speaker_id = fields[7]\n",
        "        start_time = float(fields[3])\n",
        "        end_time = start_time + float(fields[4])\n",
        "\n",
        "        # Append the speaker turn to the list\n",
        "        hyp_turns.append((speaker_id, start_time, end_time))\n",
        "\n",
        "# Compute the DER between the reference and hypothesis speaker turns\n",
        "error = simpleder.DER(ref_turns, hyp_turns)\n",
        "\n",
        "print(\"DER={:.3f}\".format(error))"
      ],
      "metadata": {
        "id": "XdSYD_rWv5nL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}